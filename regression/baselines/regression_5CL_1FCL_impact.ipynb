{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hipecta.data import ctaTelescope2Matrix\n",
    "from hipecta import core\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, sampler\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import hipecta.plots as plots\n",
    "from torchvision import transforms, utils\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "converter_path = os.path.abspath(os.path.join('/home/jacquemm/GammaLearn/converter_hdf5'))\n",
    "if converter_path not in sys.path:\n",
    "    sys.path.append(converter_path)\n",
    "    \n",
    "from converter_hdf5 import *\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModelLST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModelLST, self).__init__()\n",
    "        # conv1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # non-linearity\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # maxpooling 1, by default floor\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2) # nn.AvgPool2d\n",
    "        # batch norm\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # conv2\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # non-linearity\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # maxpooling 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        # batch norm\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # conv3\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # non-linearity\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # maxpooling 3\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2) # nn.AvgPool2d\n",
    "        # batch norm\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # conv4\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        # non-linearity\n",
    "        self.relu4 = nn.ReLU()\n",
    "        # maxpooling 4\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2) # nn.AvgPool2d\n",
    "        # batch norm\n",
    "        self.batchnorm4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # conv5\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "        # non-linearity\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        # readout, regression of energy, altitude, azimuth, xCore, yCore\n",
    "        #self.fc1 = nn.Linear(128, 5)\n",
    "         # readout, regression of xCore and yCore\n",
    "        self.fc1 = nn.Linear(128, 2)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform(m.weight.data, mode='fan_out')\n",
    "                \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.maxpool3(out)\n",
    "        out = self.batchnorm3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.maxpool4(out)\n",
    "        out = self.batchnorm4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.relu5(out)\n",
    "\n",
    "        # Reshape out from 100,128,1 to 100,128\n",
    "        out = out.view(out.size(0),-1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = '/media/data/CTA_Project/Prod3b/LaPalma_gamma_point_source_20deg_0deg_prod3b_training_0000.hdf5'\n",
    "f_1 = h5py.File(file_1, 'r', libver='latest', swmr=True)\n",
    "file_2 = '/media/data/CTA_Project/Prod3b/LaPalma_gamma_point_source_20deg_0deg_prod3b_training_0001.hdf5'\n",
    "f_2 = h5py.File(file_2, 'r', libver='latest', swmr=True)\n",
    "file_3 = '/media/data/CTA_Project/Prod3b/LaPalma_gamma_point_source_20deg_0deg_prod3b_training_0002.hdf5'\n",
    "f_3 = h5py.File(file_3, 'r', libver='latest', swmr=True)\n",
    "file_4 = '/media/data/CTA_Project/Prod3b/LaPalma_gamma_point_source_20deg_0deg_prod3b_training_0003.hdf5'\n",
    "f_4 = h5py.File(file_4, 'r', libver='latest', swmr=True)\n",
    "file_5 = '/media/data/CTA_Project/Prod3b/LaPalma_gamma_point_source_20deg_0deg_prod3b_training_0004.hdf5'\n",
    "f_5 = h5py.File(file_5, 'r', libver='latest', swmr=True)\n",
    "file_6 = '/media/data/CTA_Project/Prod3b/LaPalma_gamma_point_source_20deg_0deg_prod3b_training_0010.hdf5'\n",
    "f_6 = h5py.File(file_6, 'r', libver='latest', swmr=True)\n",
    "injTable = np.array(f_1['/Cameras/LSTCAM/injTable'])\n",
    "nbRow = f_1['/Cameras/LSTCAM'].attrs['nbRow']\n",
    "nbCol = f_1['/Cameras/LSTCAM'].attrs['nbCol']\n",
    "dataset_1 = LSTCamDataset(hdf5_file=f_1,\n",
    "                             transform=transforms.Compose([\n",
    "                                 TelescopeToSquareMatrix(injTable, nbRow, nbCol),\n",
    "                                 ToTensor()\n",
    "                             ]))\n",
    "dataset_2 = LSTCamDataset(hdf5_file=f_2,\n",
    "                             transform=transforms.Compose([\n",
    "                                 TelescopeToSquareMatrix(injTable, nbRow, nbCol),\n",
    "                                 ToTensor()\n",
    "                             ]))\n",
    "dataset_3 = LSTCamDataset(hdf5_file=f_3,\n",
    "                             transform=transforms.Compose([\n",
    "                                 TelescopeToSquareMatrix(injTable, nbRow, nbCol),\n",
    "                                 ToTensor()\n",
    "                             ]))\n",
    "dataset_4 = LSTCamDataset(hdf5_file=f_4,\n",
    "                             transform=transforms.Compose([\n",
    "                                 TelescopeToSquareMatrix(injTable, nbRow, nbCol),\n",
    "                                 ToTensor()\n",
    "                             ]))\n",
    "dataset_5 = LSTCamDataset(hdf5_file=f_6,\n",
    "                             transform=transforms.Compose([\n",
    "                                 TelescopeToSquareMatrix(injTable, nbRow, nbCol),\n",
    "                                 ToTensor()\n",
    "                             ]))\n",
    "dataset_test = LSTCamDataset(hdf5_file=f_6,\n",
    "                             transform=transforms.Compose([\n",
    "                                 TelescopeToSquareMatrix(injTable, nbRow, nbCol),\n",
    "                                 ToTensor()\n",
    "                             ]))\n",
    "\n",
    "\n",
    "dataset_train = ConcatDataset([dataset_1, dataset_2, dataset_3, dataset_4, dataset_5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Creation of subset train and test\n",
    "#random_indices = torch.randperm(len(test_dataset))\n",
    "#train_max_index = int(len(test_dataset)*0.9)\n",
    "#train_set_sampler = sampler.SubsetRandomSampler(random_indices[0:train_max_index])\n",
    "#test_set_sampler = sampler.SubsetRandomSampler(random_indices[train_max_index + 1:])\n",
    "\n",
    "batch_size = 16\n",
    "n_iters = 125000\n",
    "num_epochs = int(n_iters/(len(dataset_train)/batch_size))\n",
    "print(num_epochs)\n",
    "\n",
    "# iterable subset creation\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                          num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                         num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModelLST()\n",
    "model.double()\n",
    "onGPU = True\n",
    "\n",
    "## Run on GPU ##\n",
    "if torch.cuda.is_available() and onGPU:\n",
    "    print('model on GPU')\n",
    "    model.cuda()\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "loss_epoch = []\n",
    "itera = 0\n",
    "time_1 = time.time()\n",
    "gpu_time = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, samples in enumerate(train_loader):\n",
    "\n",
    "        ## Run on GPU ##\n",
    "        if torch.cuda.is_available() and onGPU:\n",
    "            #Load data as variable\n",
    "            images = Variable(samples['image'].cuda())\n",
    "            labels = Variable(samples['labels'].cuda())\n",
    "        else:\n",
    "            images = Variable(samples['image'])\n",
    "            labels = Variable(samples['labels'])\n",
    "        \n",
    "        time_g = time.time()\n",
    "        # clear gradient wrt parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "    \n",
    "        # claculate loss\n",
    "        loss = criterion(outputs, labels[:, 3:5])\n",
    "    \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "        gpu_time += (time.time() - time_g)\n",
    "        \n",
    "        itera +=1\n",
    "    \n",
    "        if itera%100==0:    \n",
    "            print('Epoch {} Iteration {} Training Loss {} '.format(epoch, itera, loss.data[0]))\n",
    "            #print(model.state_dict()['conv1.weight'][0])\n",
    "    time_g = time.time()\n",
    "    pos = np.empty((0, 2), np.double)\n",
    "    pos_inferred = np.empty((0, 2), np.double)\n",
    "    for i, samples in enumerate(test_loader):\n",
    "\n",
    "        ## Run on GPU ##\n",
    "        if torch.cuda.is_available() and onGPU:\n",
    "            #Load data as variable\n",
    "            images = Variable(samples['image'].cuda())\n",
    "            labels = Variable(samples['labels'].cuda())\n",
    "        else:\n",
    "            images = Variable(samples['image'])\n",
    "            labels = Variable(samples['labels'])\n",
    "        model.eval()\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        pos = np.append(pos, labels.data[:, 3:5].cpu().numpy())\n",
    "        pos_inferred = np.append(pos_inferred, outputs.data.cpu().numpy())\n",
    "    \n",
    "    loss_epoch.append(np.mean(np.abs(pos - pos_inferred)))\n",
    "    gpu_time += (time.time() - time_g)\n",
    "    print('Epoch {} Test Loss {} '.format(epoch, loss_epoch[epoch]))\n",
    "    \n",
    "ratio_gpu = gpu_time / (time.time() - time_1)\n",
    "print('Ratio of gpu time : ', ratio_gpu)\n",
    "print('Total time : ', time.time() - time_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pos = np.empty((0, 2), np.double)\n",
    "pos_inferred = np.empty((0, 2), np.double)\n",
    "\n",
    "for i, samples in enumerate(test_loader):\n",
    "\n",
    "    ## Run on GPU ##\n",
    "    if torch.cuda.is_available() and onGPU:\n",
    "        #Load data as variable\n",
    "        images = Variable(samples['image'].cuda())\n",
    "        labels = Variable(samples['labels'].cuda())\n",
    "    else:\n",
    "        images = Variable(samples['image'])\n",
    "        labels = Variable(samples['labels'])\n",
    "\n",
    "   \n",
    "    # forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    pos = np.concatenate((pos, labels.data[:,3:5].cpu().numpy()))\n",
    "    pos_inferred = np.concatenate((pos_inferred, outputs.data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'loss_epoch': loss_epoch}, f='/home/mikael/GammaLearn/model1_impactPos_kaimin-uniform_batchnorm_lr0.00005_10epochs_200000imgs.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( loss_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram definition\n",
    "bins = [100,100] # number of bins\n",
    "\n",
    "cm = plt.cm.jet\n",
    "cm.set_under('w',1)\n",
    "# histogram the data\n",
    "f, (fig1, fig2) = plt.subplots(1, 2, sharey=True)\n",
    "_, _, _, im1 = fig1.hist2d(pos[:, 0], pos_inferred[:, 0], bins=bins, cmap=cm, cmin=1)\n",
    "fig1.plot([-500, 0, 500], [-500, 0, 500],'r')\n",
    "fig1.set_title('xCore')\n",
    "fig1.set_xlabel('real position')\n",
    "fig1.set_ylabel('inferred position')\n",
    "_, _, _, im2 = fig2.hist2d(pos[:, 1], pos_inferred[:, 1], bins=bins, cmap=cm, cmin=1)\n",
    "fig2.plot([-500, 0, 500], [-500, 0, 500],'r')\n",
    "fig2.set_title('yCore')\n",
    "fig2.set_xlabel('real position')\n",
    "f.colorbar(im1, ax=fig1)   \n",
    "f.colorbar(im2, ax=fig2)\n",
    "f.set_figwidth(14)\n",
    "f.set_figheight(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1.close()\n",
    "f_2.close()\n",
    "f_3.close()\n",
    "f_4.close()\n",
    "f_5.close()\n",
    "f_6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
